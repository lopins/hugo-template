<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on Huabing Blog</title><link>https://lopins.github.io/hugo-template/tags/kubernetes/</link><description>Recent content in Kubernetes on Huabing Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 11 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://lopins.github.io/hugo-template/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>为什么 Envoy Gateway 是云原生时代的七层网关？</title><link>https://lopins.github.io/hugo-template/post/2023-04-11-why-eg-is-the-gateway-in-cloud-native-era/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2023-04-11-why-eg-is-the-gateway-in-cloud-native-era/</guid><description>&lt;h1 id="初识-envoy">初识 Envoy&lt;/h1>
&lt;p>大家好，我叫赵化冰，是 CNCF 云原生基金会大使，也是一个软件行业老兵和云原生从业者。我还记得，当我 2017 年在 Linux 基金会下的一个开源项目中从事微服务相关工作时，第一次从该项目的一个朋友那里了解到了 Istio/Envoy。从此以后，我就被 Istio/Envoy 的先进设计理念所吸引。我是国内最早一批从事 Istio/Enovy 产品研发的技术人员之一，在 2018 年就主导了 Istio/Envoy 的第一个产品化项目。在后续的工作中，我还研发了大规模 Kubernetes 集群上基于 Envoy 的多租户七层云原生网关，创建了基于 Envoy 的多协议七层网关开源项目 MetaProtocolProxy，以及基于 Envoy/Istio 的多协议服务网格开源项目 Aeraki Mesh（CNCF Sandbox 项目），该项目被腾讯、百度、华为等多个公司采用，在基于 Envoy 的网关和服务网格上支持了超过数十种应用协议。今天，我想和大家聊一聊 Envoy 生态中的新成员 Envoy Gateway，以及为什么我认为 Envoy Gateway 是云原生时代的七层网关。&lt;/p></description></item><item><title>Kubernetes Controller 机制详解（二）</title><link>https://lopins.github.io/hugo-template/post/2023-04-04-how-to-create-a-k8s-controller-2/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2023-04-04-how-to-create-a-k8s-controller-2/</guid><description>&lt;p>在上一篇文章 &lt;a href="https://www.zhaohuabing.com/post/2023-03-09-how-to-create-a-k8s-controller/">Kubernetes Controller 机制详解（一）&lt;/a>中，我们学习了 Kubernetes API List/Watch 机制，以及如何采用 Kubernetes client-go 中的 Informer 机制来创建 Controller。该方法需要用户了解 Kubernetes client-go 的实现原理，并在 Controller 的编码中处理较多 Informer 实现相关的细节。包括启动 InformerFactory，将 Watch 到的消息加入到队列，重试等等逻辑。如果有多个副本，还需要加入 Leader Election 的相关代码。如果需如果你创建了自定义的 CRD，可能还希望在创建资源时采用 webhook 对资源进行校验。这些功能都需要用户编写较多的代码。&lt;/p></description></item><item><title>Kubernetes Controller 机制详解（一）</title><link>https://lopins.github.io/hugo-template/post/2023-03-09-how-to-create-a-k8s-controller/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2023-03-09-how-to-create-a-k8s-controller/</guid><description>&lt;p>Kubernetes(简称K8s) 是一套容器编排和管理系统，可以帮助我们部署、扩展和管理容器化应用程序。在 K8s 中，Controller 是一个重要的组件，它可以根据我们的期望状态和实际状态来进行调谐，以确保我们的应用程序始终处于所需的状态。本系列博文将解析 K8s Controller 的实现机制，并介绍如何编写一个 Controller。&lt;/p></description></item><item><title>How to Pass the Certified Kubernetes Administrator (CKA) Exam Without Any Stress?</title><link>https://lopins.github.io/hugo-template/post/2022-02-08-how-to-prepare-cka-en/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2022-02-08-how-to-prepare-cka-en/</guid><description>&lt;p>The CKA exam is not that hard. You can pass the CKA exam in less than 3 months without any stress if you follow the steps in this post to prepare. You have my word. I’ve tried myself and just successfully get my CKA certificate!&lt;/p>
&lt;h1 id="how-much-time-do-i-need-to-prepare-for-the-cka-exam">How much time do I need to prepare for the CKA exam?&lt;/h1>
&lt;p>It depends, an experienced DevOps engineer could spend much less time than a newbie to prepare, but still get a high score in the exam. In general, I suggest spending 30 minutes to 1 hour to practice each day, and the preparation could last for 3 months if you don’t have much experience in K8s yet, or less if you have already worked with K8s for some time.&lt;/p></description></item><item><title>如何成功通过 CKA 考试？</title><link>https://lopins.github.io/hugo-template/post/2022-02-08-how-to-prepare-cka/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2022-02-08-how-to-prepare-cka/</guid><description>&lt;h1 id="了解-cka-考察的内容">了解 CKA 考察的内容&lt;/h1>
&lt;p>在开始准备考试前一定要阅读&lt;a href="https://github.com/cncf/curriculum">CNCF 官方考试大纲&lt;/a>，了解 CKA 考察考生的主要内容，以在备考时做到知己知彼，有的放矢，根据该考试大纲进行针对性的准备和练习。该大纲会根据 K8s 的版本进行更新，但每个版本中涉及的考试内容变化不大，下面是我准备考试时的版本（v1.22）要求的主要内容：&lt;/p></description></item><item><title>Istio 知识图谱</title><link>https://lopins.github.io/hugo-template/post/2020-04-02-istio-mindmap/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2020-04-02-istio-mindmap/</guid><description>&lt;p>&lt;a href="https://lopins.github.io/hugo-template/mindmap/istio.html">Mind Map&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Istio
&lt;ul>
&lt;li>流量管理
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/">Istio流量管理实现机制深度解析
&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-12-07-cnbps2020-istio-traffic-management/">Istio 流量管理原理与协议扩展&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>可见性
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-06-22-using-opentracing-with-istio/">实现方法级调用跟踪&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-07-02-using-opentracing-with-istio/">实现 Kafka 消息调用跟踪&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>协议扩展
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2021-03-02-manage-any-layer-7-traffic-in-istio/">如何在 Isito 中支持 Dubbo、Thrift、Redis，以及任何七层协议？&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-10-14-redis-cluster-with-istio/">在 Istio 中实现 Redis 集群的数据分片、读写分离和流量镜像&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/aeraki-mesh/aeraki">Aeraki: Manage any layer 7 traffic in an Istio service mesh&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>故障定位
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-09-11-headless-mtls/">Headless Service&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-09-05-istio-sidecar-dependency/">Sidecar 启动依赖&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://tencentcloudcontainerteam.github.io/tke-handbook/skill/capture-packets-in-container.html">Pod 内抓包&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>源码分析
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-10-21-pilot-discovery-code-analysis/">Pilot 源码解析&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-02-18-pilot-service-registry-code-analysis/">Istio 服务注册机制&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2018-10-29-envoy-build/">Envoy Proxy 构建分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/2018/05/23/istio-auto-injection-with-webhook/">Sidecar 自动注入&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>腾讯云容器网络介绍</title><link>https://lopins.github.io/hugo-template/post/2021-03-24-tke-network-mode/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2021-03-24-tke-network-mode/</guid><description>&lt;p>K8s 对于集群 Pod 的网络模型只有下面两点简单的要求：&lt;/p>
&lt;ul>
&lt;li>节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信&lt;/li>
&lt;li>节点上的代理（比如：系统守护进程、kubelet）可以和节点上的所有 Pod 通信&lt;/li>
&lt;/ul>
&lt;p>在实现该网络模型时，为了应对不同的使用场景，TKE（Tencent Kubernetes Engine）提供了 Global Router 和 VPC-CNI 两种网络模式。本文中，我们将通过这两种模式下数据包的转发流程来分析这两种模式各自的实现原理。本文还会对比分析不同网络模式下的网络效率和资源使用情况，以便于大家在创建 TKE 集群时根据应用对网络的需求和使用成本选择合适的网络模型。&lt;/p></description></item><item><title>一文带你彻底厘清 Kubernetes 中的证书工作机制</title><link>https://lopins.github.io/hugo-template/post/2020-05-19-k8s-certificate/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2020-05-19-k8s-certificate/</guid><description>&lt;p>接触 Kubernetes 以来，我经常看到 Kubernetes 在不同的地方使用了证书（Certificate），在 Kubernetes 安装和组件启动参数中也需要配置大量证书相关的参数。但是 Kubernetes 的文档在解释这些证书的工作机制方面做得并不是太好。经过大量的相关阅读和分析工作后，我基本弄清楚了 Kubernetes 中证书的使用方式。在本文中，我将试图以一种比官方文档更容易理解的方式来说明 Kubernetes 中证书相关的工作机制，如果你也存在这方面的疑惑，希望这篇文章对你有所帮助。&lt;/p></description></item><item><title>Kubernetes 知识图谱</title><link>https://lopins.github.io/hugo-template/post/2020-02-22-k8s-mindmap/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2020-02-22-k8s-mindmap/</guid><description>&lt;p>&lt;a href="https://lopins.github.io/hugo-template/mindmap/k8s.html">Mind Map&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Kubernetes
&lt;ul>
&lt;li>基本理念
&lt;ul>
&lt;li>自动化部署，缩扩容和管理容器应用&lt;/li>
&lt;li>预期状态管理(Desired State Management)
&lt;ul>
&lt;li>Kubernetes API 对象（声明预期状态）&lt;/li>
&lt;li>Kubernetes Control Plane（确保集群当前状态匹配预期状态）
&lt;ul>
&lt;li>Kubernetes Master
&lt;ul>
&lt;li>kube-apiserver（API Server）
&lt;ul>
&lt;li>对外提供各种对象的CRUD REST接口&lt;/li>
&lt;li>对外提供Watch机制，通知对象变化&lt;/li>
&lt;li>将对象存储到Etcd中&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>kube-controller-manager（守护进程）
&lt;ul>
&lt;li>功能：通过apiserver监视集群的状态，并做出相应更改，以使得集群的当前状态向预期状态靠拢&lt;/li>
&lt;li>controllers
&lt;ul>
&lt;li>replication controller&lt;/li>
&lt;li>endpoints controller&lt;/li>
&lt;li>namespace controller&lt;/li>
&lt;li>serviceaccounts controller&lt;/li>
&lt;li>&amp;hellip;&amp;hellip;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>kube-scheduler（调度器）
&lt;ul>
&lt;li>功能：将Pod调度到合适的工作节点上运行&lt;/li>
&lt;li>调度的考虑因素
&lt;ul>
&lt;li>资源需求&lt;/li>
&lt;li>服务治理要求&lt;/li>
&lt;li>硬件/软件/策略限制&lt;/li>
&lt;li>亲和以及反亲和要求&lt;/li>
&lt;li>数据局域性&lt;/li>
&lt;li>负载间的干扰&lt;/li>
&lt;li>&amp;hellip;&amp;hellip;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Work Node
&lt;ul>
&lt;li>Kubelet（节点代理）
&lt;ul>
&lt;li>接受通过各种机制（主要是通过apiserver）提供的一组PodSpec&lt;/li>
&lt;li>确保PodSpec中描述的容器处于运行状态且运行状况良好&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Kube-proxy（节点网络代理）
&lt;ul>
&lt;li>在节点上提供Kubernetes API中定义Service&lt;/li>
&lt;li>设置Service对应的IPtables规则&lt;/li>
&lt;li>进行流量转发（userspace模式）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>部署模式
&lt;ul>
&lt;li>Single node&lt;/li>
&lt;li>Single head node，multiple workers
&lt;ul>
&lt;li>API Server，Scheduler，and Controller Manager run on a single node&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Single etcd，HA heade nodes，multiple workers
&lt;ul>
&lt;li>Multiple API Server instances fronted by a load balancer&lt;/li>
&lt;li>Multiple Scheduler and Controller Manager instances with leader election&lt;/li>
&lt;li>Single etcd node&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>HA etcd，HA head nodes，multiple workers
&lt;ul>
&lt;li>Multiple API Server instances fronted by a load balancer&lt;/li>
&lt;li>Multiple Scheduler and Controller Manager instances with leader election&lt;/li>
&lt;li>Etcd cluster run on nodes seperate from the Kubernetes head nodes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Kubernetes Federation&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>商业模式
&lt;ul>
&lt;li>云服务用户：避免使用单一云提供商导致的厂商锁定，避免技术和成本风险&lt;/li>
&lt;li>云服务厂商：使用Kubernetes来打破AWS的先入垄断地位，抢夺市场份额&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Workload
&lt;ul>
&lt;li>Pod
&lt;ul>
&lt;li>Smalleset deployable computing unit
- Consist of one or more containers
- All containers in a pod share &lt;a href="https://kubernetes.io/docs/concepts/storage/volumes/">storage&lt;/a>, &lt;a href="https://zhaohuabing.com/post/2020-03-12-linux-network-virtualization/#network-namespace">network namespacem&lt;/a> and &lt;a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">cgroup&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Workload resources(Controllers)
&lt;ul>
&lt;li>Deployment &amp;amp; RelicaSet
&lt;ul>
&lt;li>Deployment is used to deploy stateless appliations.&lt;/li>
&lt;li>ReplicaSet ensured a specified numbers of pod replicas are running at a given time.&lt;/li>
&lt;li>Deployment is used to rollout/update/rollback ReplicaSet.&lt;/li>
&lt;li>ReplicaSet is not supposed to be used directly, it should be managed by Deployments.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>StatefulSet
&lt;ul>
&lt;li>StatefulSet is used to deploy stateful applications.&lt;/li>
&lt;li>SetatefSet require a Headless Service to provide network identity for the pods.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>DaemonSet
&lt;ul>
&lt;li>DaemonSet ensures that all(or some) Nodes run a copy of a Pod.&lt;/li>
&lt;li>Use cases: cluster storage daemon, logs collection daemon, node monitoring daemon.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Job &amp;amp; CronJob
&lt;ul>
&lt;li>Job runs pods until a specified number of them have been succcessfully executed.&lt;/li>
&lt;li>CronJob runs a job periodically on a given schedule.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Storage
&lt;ul>
&lt;li>Volume
&lt;ul>
&lt;li>purpose
&lt;ul>
&lt;li>Persist data across the life span of a Pod
&lt;ul>
&lt;li>Data won&amp;rsquo;t lost when a container is restarted&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Share data between containers running together in a Pod
&lt;ul>
&lt;li>Volume can be mounted to mutiple containers inside a Pod&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>type
&lt;ul>
&lt;li>configMap&lt;/li>
&lt;li>emptyDir&lt;/li>
&lt;li>hostPath&lt;/li>
&lt;li>local&lt;/li>
&lt;li>persistentVolumeClaim&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Policies
&lt;ul>
&lt;li>ResourceQuota
&lt;ul>
&lt;li>purpose
&lt;ul>
&lt;li>Limit the aggregated resource consumption of a Namespace&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Scope
&lt;ul>
&lt;li>Namespaced: ResourceQuota is enforced in a Namespace scope, different Namespaces have different Resouce limit&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Type
&lt;ul>
&lt;li>Compute Resource Quota
&lt;ul>
&lt;li>CPU (limits.cpu requests.cpu)&lt;/li>
&lt;li>Memory (limits.memory requets.memory)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Storage Resource Quota
&lt;ul>
&lt;li>Persistent Storage (storage)&lt;/li>
&lt;li>Ephemeral Storage (ephermal-storage)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Object Count Quota
&lt;ul>
&lt;li>Limit of total number of Namespaced resources (count/services)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Request and Limit
&lt;ul>
&lt;li>Request: Resources that are guaranteed to get&lt;/li>
&lt;li>Limit: The maximum amount of resources that one can get&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Network
&lt;ul>
&lt;li>Linux Network Virtualization
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-02-24-linux-taptun/">Linux tun/tap&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-03-12-linux-network-virtualization/#network-namespace">Network Namespace&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-03-12-linux-network-virtualization/#veth">Veth Pair&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-03-12-linux-network-virtualization/#bridge">Linux bridge&lt;/a>&lt;/li>
&lt;li>Vlan&lt;/li>
&lt;li>Vxlan
&lt;ul>
&lt;li>&lt;a href="https://cizixs.com/2017/09/25/vxlan-protocol-introduction/">Vxlan原理&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cizixs.com/2017/09/28/linux-vxlan/">Linux 上实现 vxlan 网络&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Routing Protocol
&lt;ul>
&lt;li>Distance Vector Protocol
&lt;ul>
&lt;li>BGP&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Link-State Protocol
&lt;ul>
&lt;li>OSPF&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>K8s Network
&lt;ul>
&lt;li>Service
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-03-29-how-to-choose-ingress-for-service-mesh/#cluster-ip">Cluster IP&lt;/a>
&lt;ul>
&lt;li>Provides access in the cluster internally&lt;/li>
&lt;li>The ClusterIP range is defined in API server startup option &lt;code>-service-cluster-ip-range&lt;/code>&lt;/li>
&lt;li>Service port is defined in the Service Manifest&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-03-29-how-to-choose-ingress-for-service-mesh/#nodeport">NodePort&lt;/a>
&lt;ul>
&lt;li>Provides access at the node level&lt;/li>
&lt;li>The NodePort range is defined in API server startup option &lt;code>--service-node-port-range&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-03-29-how-to-choose-ingress-for-service-mesh/#loadbalancer">LoadBalancer&lt;/a>
&lt;ul>
&lt;li>Provides an external IP to allow access from outside of the cluster&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#externalname">ExternalName&lt;/a>
&lt;ul>
&lt;li>An alias to an external service&lt;/li>
&lt;li>DNS redirection&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-09-11-headless-mtls/#%E4%BB%80%E4%B9%88%E6%98%AF%E6%97%A0%E5%A4%B4%E6%9C%8D%E5%8A%A1">Headless&lt;/a>
&lt;ul>
&lt;li>Define a Headless service: specify &amp;ldquo;None&amp;rdquo; in for the cluster IP(.spec.clusterIP)&lt;/li>
&lt;li>No cluster IP allocated to Headless services&lt;/li>
&lt;li>No load balancing and proxying for Headless service&lt;/li>
&lt;li>Kube dns returns the IP of the pods backing the service&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy">Kube Proxy&lt;/a>
&lt;ul>
&lt;li>Provides a proxy server or appliction-level gateway between localhost and the K8s API server&lt;/li>
&lt;li>Handles locating the apiserver and authenticating (uses cluster configuration and user credential in .kube/config)&lt;/li>
&lt;li>Can send requests to API server (for example: get the list of services in default namespace &lt;code>localhost:proxy-port/api/v1/namespaces/default/services&lt;/code>)&lt;/li>
&lt;li>Can send requests to services via url &lt;code>localhost:proxy-port/api/v1/namespaces/namespace_name/services/service_name[:port_name]/proxy/[application url] &lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Kubectl port-forward
&lt;ul>
&lt;li>Forward local ports to a pod&lt;/li>
&lt;li>kebectl port-forward deployment/mydeployment localport:port&lt;/li>
&lt;li>kebectl port-forward service/myservice localport:port&lt;/li>
&lt;li>kebectl port-forward pod/mypod localport:port&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ingress
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-03-29-how-to-choose-ingress-for-service-mesh/#k8s-ingress">K8s Ingress&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-03-29-how-to-choose-ingress-for-service-mesh/#istio-gateway">Istio Ingress Gateway&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>DNS
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#services">Service&lt;/a>
&lt;ul>
&lt;li>Normal Service
&lt;ul>
&lt;li>A/AAA record which resolves name to the Cluster IP
&lt;ul>
&lt;li>Name: &lt;code>my-svc.my-namespace.svc.cluster-domain.example&lt;/code>&lt;/li>
&lt;li>Example: &lt;code>kubernetes.default.svc.cluster.local. 30 IN A	172.20.252.11&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>SRV record for each named service port
&lt;ul>
&lt;li>Name: &lt;code>_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.&lt;/code>example&lt;/li>
&lt;li>Example: &lt;code>_https._tcp.kubernetes.default.svc.cluster.local. 5 IN SRV 0 100 443 kubernetes.default.svc.cluster.local.&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>A PTR record which resolves Cluster IP to domain name
&lt;ul>
&lt;li>Example &lt;code>1.252.20.172.in-addr.arpa. 5	IN	PTR	kubernetes.default.svc.cluster.local.&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Headless Service
&lt;ul>
&lt;li>A/AAA record which resolves to the set of IPs of the pods selected by the service&lt;/li>
&lt;li>N*M SRV records (N pods, M named ports in service)&lt;/li>
&lt;li>A PTR record which resolves pod IP to domain name of each pod&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ExternalName
&lt;ul>
&lt;li>A CNAME pointing to the domain name of the external service&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Pod
&lt;ul>
&lt;li>A/AAA record which resolves name to the pod IP&lt;/li>
&lt;li>General name
&lt;ul>
&lt;li>Name: &lt;code>pod-ip-address.my-namespace.pod.cluster-domain.example&lt;/code>&lt;/li>
&lt;li>Example: &lt;code>172-20-0-57.default.pod.cluster.local. 3 IN A	172.20.0.57&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Pod created by Deployment or DaemonSet exposed by a Service
&lt;ul>
&lt;li>&lt;code>pod-ip-address.deployment-name.my-namespace.svc.cluster-domain.example&lt;/code>&lt;/li>
&lt;li>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#coredns">CoreDNS&lt;/a>
&lt;ul>
&lt;li>Plugins
&lt;ul>
&lt;li>errors: Erros are logged to stdout&lt;/li>
&lt;li>prometheus: Metrics of CoreDNS are available at &lt;code>http://localhost:9153/metrics&lt;/code> in Prometheus format&lt;/li>
&lt;li>&lt;a href="https://coredns.io/plugins/kubernetes/">kubernetes&lt;/a>: CoreDNS will reply to DNS queries based on IP of the services and pods of Kubernetes.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>腾讯云
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2021-03-24-tke-network-mode/#global-router-%E6%A8%A1%E5%BC%8F">Global Router&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2021-03-24-tke-network-mode/#vpc-cni-%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F">VPC-CNI&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2019-03-29-how-to-choose-ingress-for-service-mesh/#api-gateway--sidecar-proxy">API Gateway+Service Mesh&lt;/a>&lt;/li>
&lt;li>Kubernetes CNI插件
&lt;ul>
&lt;li>&lt;a href="https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/04/11/calico-usage.html">Calico&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Scheduling
&lt;ul>
&lt;li>Algorithm: Predicate find a set of available nodes -&amp;gt; Priority select the best suitable node
&lt;ul>
&lt;li>Predicates: find available nodes through some conditions: check memory, cpu, disk, etc.&lt;/li>
&lt;li>Priorities: select a node to run the scheduled pod: select the node with the least amount of pods by default&lt;/li>
&lt;li>Policy: specify a number of predicates and priorities&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Run a customscheduler
&lt;ul>
&lt;li>Policy: &lt;code>--policy-config-file&lt;/code>&lt;/li>
&lt;li>Name: &lt;code>--scheduler-name&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Pod Specification: hits for pod scheduling
&lt;ul>
&lt;li>NodeName: assign pods to the named node&lt;/li>
&lt;li>NodeSelector: assign pods to a group of nodes with particular labels&lt;/li>
&lt;li>Affinity and anti-affinity:
&lt;ul>
&lt;li>Node
&lt;ul>
&lt;li>Node affinity: has the same ability to constrain pods to particular nodes, but is more expressive and powerful&lt;/li>
&lt;li>Node anti-affinity: use &lt;code>NotIn&lt;/code> and &lt;code>DoesNotExist&lt;/code> to achieve node anti-affinity&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Inter-Pod
&lt;ul>
&lt;li>Inter-Pod affinity: co-locate some pods in the same nodes&lt;/li>
&lt;li>Inter-Pod anti-affinity: distribute some pods in different nodes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>taints and tolerations
&lt;ul>
&lt;li>allow a node to repel a set of pods&lt;/li>
&lt;li>allow pods to be scheduled onto nodes with matching taints&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>SchedulerName: choose a specific scheduler to schedule a pod&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Security
&lt;ul>
&lt;li>Background Knowledge
&lt;ul>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-03-19-pki/">Certificate and PKI&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-05-19-k8s-certificate/">Kubernetes 中使用到的证书&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>User Type
&lt;ul>
&lt;li>Service Account
&lt;ul>
&lt;li>Managed by Kubernetes&lt;/li>
&lt;li>Represent workloads in the cluster&lt;/li>
&lt;li>Bound to a specific namespace&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user">Normal User&lt;/a>
&lt;ul>
&lt;li>Managed out side of Kubernetes&lt;/li>
&lt;li>Authenticated with a valid certicated signed by the cluster&amp;rsquo;s CA
&lt;ul>
&lt;li>User name: Certificate subject &lt;a href="https://docs.oracle.com/cd/E24191_01/common/tutorials/authz_cert_attributes.html">Common Name&lt;/a> field&lt;/li>
&lt;li>Group: Certificate subject &lt;a href="https://docs.oracle.com/cd/E24191_01/common/tutorials/authz_cert_attributes.html">Organization&lt;/a> field&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Authentication
&lt;ul>
&lt;li>Service account tokens for service accounts&lt;/li>
&lt;li>Client certifications for normal users&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-05-19-k8s-certificate/#service-account--%E8%AF%81%E4%B9%A6">Certifications for control plane components communication&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhaohuabing.com/post/2020-05-19-k8s-certificate/#%E4%BD%BF%E7%94%A8-tls-bootstrapping-%E7%AE%80%E5%8C%96-kubelet-%E8%AF%81%E4%B9%A6%E5%88%B6%E4%BD%9C">Bootstrap Token&lt;/a> for clusters and nodes bootstrapping&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Authorization
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC&lt;/a>
&lt;ul>
&lt;li>Namespace Scope
&lt;ul>
&lt;li>Role&lt;/li>
&lt;li>RoleBinding (Associate users retrived from authentication process to Roles)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Cluster Scope
&lt;ul>
&lt;li>ClusterRole&lt;/li>
&lt;li>CluseterRoleBinding (Associate users retrived from authentication process to ClusteRoles)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Helm: package management tool for K8s applications
&lt;ul>
&lt;li>Chart: package all k8s manifests as a single tarball
&lt;ul>
&lt;li>Chart.yaml: this files contains metadata about this Chart: name, version, keywords&lt;/li>
&lt;li>templeates: this directorey contains the resource manifests that makes up this application
&lt;ul>
&lt;li>deployment&lt;/li>
&lt;li>services&lt;/li>
&lt;li>secretes&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>values.yaml: this files contains keys and values that are used to generate the release. These values are replaced in the resource manifests using the Go template syntax&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Repository: HTTP servers that contains charts&lt;/li>
&lt;li>Helm commands
&lt;ul>
&lt;li>helm search hub redis: find redis chart and its repository in helm hub&lt;/li>
&lt;li>helm sarch repo redis: find redis chart in repositories&lt;/li>
&lt;li>helm install redis bitnami/redis: install redis chart&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Extending the Kubernetes API
&lt;ul>
&lt;li>Custom Resource
&lt;ul>
&lt;li>CRD: Define custom resources&lt;/li>
&lt;li>Custom Resources/Ojbects: Declare the desired spec of a custom resource&lt;/li>
&lt;li>Custom Controllers: watch-loop to make sure the actual state meet the declared spec&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">Aggregated API Server&lt;/a>
&lt;ul>
&lt;li>Deploy an extension API server&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/">Register APIService objects&lt;/a>
&lt;ul>
&lt;li>Group: API groups this extension API server hosts&lt;/li>
&lt;li>Version: API version this extension API server hosts&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>kube-apiserver proxies client requests to the extension API server&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Which One is the Right Choice for the Ingress Gateway of Your Service Mesh?</title><link>https://lopins.github.io/hugo-template/post/2019-04-16-how-to-choose-ingress-for-service-mesh-english/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2019-04-16-how-to-choose-ingress-for-service-mesh-english/</guid><description>&lt;p>By default, in a Kubernetes cluster with the Istio service mesh enabled, services can only be accessed inside the cluster. However, some of the services may need to be exposed to external networks as well. Kubernetes and Istio provide a variety of means to get external traffic into your cluster including NodePort, LoadBalancer, Kubernetes Ingress and Istio Gateway. With all these options, which one should be the right choice for your service mesh running in production?&lt;/p></description></item><item><title>如何为服务网格选择入口网关？</title><link>https://lopins.github.io/hugo-template/post/2019-03-29-how-to-choose-ingress-for-service-mesh/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2019-03-29-how-to-choose-ingress-for-service-mesh/</guid><description>&lt;p>在启用了Istio服务网格的Kubernetes集群中，缺省情况下只能在集群内部访问网格中的服务，要如何才能从外部网络访问这些服务呢？ Kubernetes和Istio提供了NodePort，LoadBalancer，Kubernetes Ingress，Istio Gateway等多种外部流量入口的方式，面对这么多种方式，我们在产品部署中应该如何选择？&lt;/p></description></item><item><title>拥抱NFV，Istio 1.1 将支持多网络平面</title><link>https://lopins.github.io/hugo-template/post/2018-12-19-multi-network-interfaces-for-istio/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/post/2018-12-19-multi-network-interfaces-for-istio/</guid><description>&lt;p>Istio 1.0版本只支持在单个网络，即Mesh中的服务只能连接在一个网络上。虽然在架构设计上是开放的，但从目前的代码来看，Istio的内部实现还是和Kubernetes高度集成的。由于Kubernetes集群中Pod缺省只支持一个网络接口，因此Istio也存在该限制并不让人意外。&lt;/p></description></item><item><title>Istio Sidecar自动注入原理</title><link>https://lopins.github.io/hugo-template/2018/05/23/istio-auto-injection-with-webhook/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/2018/05/23/istio-auto-injection-with-webhook/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;hr>
&lt;p>Kubernets 1.9版本引入了Admission Webhook(web 回调)扩展机制，通过Webhook,开发者可以非常灵活地对Kubernets API Server的功能进行扩展，在API Server创建资源时对资源进行验证或者修改。&lt;/p>
&lt;p>使用webhook的优势是不需要对API Server的源码进行修改和重新编译就可以扩展其功能。插入的逻辑实现为一个独立的web进程，通过参数方式传入到kubernets中，由kubernets在进行自身逻辑处理时对扩展逻辑进行回调。&lt;/p>
&lt;p>Istio 0.7版本就利用了Kubernets webhook实现了sidecar的自动注入。&lt;/p></description></item><item><title>Helm介绍</title><link>https://lopins.github.io/hugo-template/2018/04/16/using-helm-to-deploy-to-kubernetes/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/2018/04/16/using-helm-to-deploy-to-kubernetes/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;hr>
&lt;p>Helm是Kubernetes生态系统中的一个软件包管理工具。本文将介绍为何要使用Helm进行Kubernetes软件包管理，澄清Helm中使用到的相关概念，并通过一个具体的示例学习如何使用Helm打包，分发，安装，升级及回退Kubernetes应用。&lt;/p></description></item><item><title>如何从外部访问Kubernetes集群中的应用？</title><link>https://lopins.github.io/hugo-template/2017/11/28/access-application-from-outside/</link><pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate><guid>https://lopins.github.io/hugo-template/2017/11/28/access-application-from-outside/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>我们知道，kubernetes的Cluster Network属于私有网络，只能在cluster Network内部才能访问部署的应用，那如何才能将Kubernetes集群中的应用暴露到外部网络，为外部用户提供服务呢？本文探讨了从外部网络访问kubernetes cluster中应用的几种实现方式。&lt;/p></description></item></channel></rss>